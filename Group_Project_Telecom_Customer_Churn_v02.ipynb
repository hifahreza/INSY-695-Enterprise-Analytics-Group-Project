{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d605785",
   "metadata": {},
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48432f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20672/3950210417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mLearningCurve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[1;33m from imblearn.over_sampling import (\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mSMOTE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADASYN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "#Import dataset with key telecom customer data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_val_predict\n",
    ")\n",
    "from yellowbrick.classifier import (\n",
    "    ConfusionMatrix,\n",
    "    ROCAUC\n",
    ")\n",
    "from yellowbrick.model_selection import (\n",
    "    LearningCurve \n",
    ")\n",
    "from imblearn.over_sampling import (\n",
    "    SMOTE, ADASYN\n",
    ")\n",
    "from seaborn import diverging_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ec850",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793fb95a",
   "metadata": {},
   "source": [
    "## Take a Quick Look at the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e981b",
   "metadata": {},
   "source": [
    "Note that the \"Total night charge\" variable is missing values. We will have to impute these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5f6e",
   "metadata": {},
   "source": [
    "### Create histograms of numberical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d16bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58ff35",
   "metadata": {},
   "source": [
    "## Create a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3157346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For illustration only. Sklearn has train_test_split()\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(data, 0.2)\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad34d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941707bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_id = data.reset_index()   # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(data_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ed203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffed8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661b883",
   "metadata": {},
   "source": [
    "### Create Histogram of target variable - Body Mass Index (BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b97c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert categorical target variable, Churn, into a numerical binary varible\n",
    "\n",
    "churn_mapper = {True:1, False:0}\n",
    "\n",
    "data[\"Churn\"] = data[\"Churn\"].replace(churn_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3180bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e415776",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Churn\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9434a29",
   "metadata": {},
   "source": [
    "### Exploring the U.S. state the customers come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"State\"].hist()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"State\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "\n",
    "strat_test_set[\"State\"].value_counts() / len(strat_test_set)\n",
    "\n",
    "data[\"State\"].value_counts() / len(data)\n",
    "\n",
    "def state_proportions(data):\n",
    "    return data[\"State\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": state_proportions(data),\n",
    "    \"Stratified\": state_proportions(strat_test_set),\n",
    "    \"Random\": state_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42167faa",
   "metadata": {},
   "source": [
    "# Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind=\"scatter\", x=\"Total_intl_calls\", y=\"Total_night_calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c646201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot\n",
    "calls_by_plan_type = sns.catplot(x=\"Total_day_calls\", col=\"International_plan\", col_wrap=4,\n",
    "                        data=data[data.Total_day_calls.notnull()],\n",
    "                        kind=\"count\", height=3.5, aspect=.8, \n",
    "                        palette='tab20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd642642",
   "metadata": {},
   "source": [
    "## Looking for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"Churn\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db902f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471968ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix, cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a843e",
   "metadata": {},
   "source": [
    "Note that there is very high correlation between the minutes and charge variables. This is becuase charge is a factor of minutes since the customers are charged a rate per minute. Therefore we need to remove one of the varibales for each pair of correlated variables before we use these variables as predictors in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2eb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"Churn\", \"Total_eve_calls\", \"Total_night_calls\",\n",
    "              \"Total_intl_minutes\", \"Total_day_minutes\", \"Total_intl_calls\"]\n",
    "scatter_matrix(data[attributes], figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8db0a",
   "metadata": {},
   "source": [
    "## Experimenting with Attribute Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfde25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Total_calls\"] = data[\"Total_day_calls\"]+data[\"Total_eve_calls\"]+data['Total_night_calls']+data['Total_intl_calls']\n",
    "data[\"Total_mins\"] = data[\"Total_day_minutes\"]+data[\"Total_eve_minutes\"]+data['Total_night_minutes']+data['Total_intl_minutes']\n",
    "data[\"Total_charges\"] = data[\"Total_day_charge\"]+data[\"Total_eve_charge\"]+data['Total_night_charge']+data['Total_intl_charge']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d710c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix[\"Churn\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb20d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the three attributes we added by combining existing attributes because we will add them in later in script\n",
    "# using the custom transformer function:\n",
    "\n",
    "data = data.drop(columns=['Total_calls', 'Total_mins', 'Total_charges'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a172f",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = strat_train_set.drop(\"Churn\", axis=1) # drop labels for training set\n",
    "data_labels = strat_train_set[\"Churn\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55a49d",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c57c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows = data[data.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504ad1c",
   "metadata": {},
   "source": [
    "We need to impute the missing values for the Total_night_charge variable. We can do this using the median value from this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7a927",
   "metadata": {},
   "source": [
    "Remove the text attribute because median can only be calculated on numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8f5ef",
   "metadata": {},
   "source": [
    "Check that this is the same as manually computing the median of each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fa20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd010d",
   "metadata": {},
   "source": [
    "Transform the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34514c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = pd.DataFrame(X, columns=data_num.columns,\n",
    "                          index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.loc[sample_incomplete_rows.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa30c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = pd.DataFrame(X, columns=data_num.columns,\n",
    "                          index=data_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e56884",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d130751",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10171692",
   "metadata": {},
   "source": [
    "Now let's preprocess the categorical input feature, `State, Area_code, International_plan, and Voice_mail_plan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35323d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat = data[[\"State\", \"Area_code\", \"International_plan\", \"Voice_mail_plan\"]]\n",
    "data_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a59a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "data_cat_encoded = ordinal_encoder.fit_transform(data_cat)\n",
    "data_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "data_cat_1hot = cat_encoder.fit_transform(data_cat)\n",
    "data_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f03868",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a333ee",
   "metadata": {},
   "source": [
    "## Custom Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b992481",
   "metadata": {},
   "source": [
    "Let's create a custom transformer to add extra attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = \"Total_day_calls\", \"Total_eve_calls\", \"Total_night_calls\", \"Total_intl_calls\", \"Total_day_minutes\", \"Total_eve_minutes\", \"Total_night_minutes\", \"Total_intl_minutes\", \"Total_day_charge\", \"Total_eve_charge\", \"Total_night_charge\", \"Total_intl_charge\"\n",
    "Total_day_calls_ix, Total_eve_calls_ix, Total_night_calls_ix, Total_intl_calls_ix, Total_day_minutes_ix, Total_eve_minutes_ix,Total_night_minutes_ix, Total_intl_minutes_ix, Total_day_charge_ix, Total_eve_charge_ix, Total_night_charge_ix, Total_intl_charge_ix = [\n",
    "    data.columns.get_loc(c) for c in col_names] # get the column indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33922ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_Total_calls=True): # no *args or **kargs\n",
    "        self.add_Total_calls = add_Total_calls\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        Total_mins = X[:, Total_day_minutes_ix] + X[:, Total_eve_minutes_ix] + X[:, Total_night_minutes_ix] + X[:, Total_intl_minutes_ix]\n",
    "        Total_charges = X[:, Total_day_charge_ix] + X[:, Total_eve_charge_ix] + X[:, Total_night_charge_ix] + X[:, Total_intl_charge_ix]\n",
    "        if self.add_Total_calls:\n",
    "            Total_calls = X[:, Total_day_calls_ix] + X[:, Total_eve_calls_ix] + X[:, Total_night_calls_ix] + X[:, Total_intl_calls_ix]\n",
    "            return np.c_[X, Total_calls, Total_mins,\n",
    "                         Total_charges]\n",
    "        else:\n",
    "            return np.c_[X, Total_mins, Total_charges]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_Total_calls=True)\n",
    "data_extra_attribs = attr_adder.transform(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b79b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_extra_attribs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77f7b4",
   "metadata": {},
   "source": [
    "Also, `data_extra_attribs` is a NumPy array, we've lost the column names (unfortunately, that's a problem with Scikit-Learn). To recover a `DataFrame`, you can run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b85089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extra_attribs = pd.DataFrame(\n",
    "    data_extra_attribs,\n",
    "    columns=list(data.columns)+[\"Total_calls\",\"Total_mins\",\"Total_charges\"],\n",
    "    index=data.index)\n",
    "data_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40e9e9",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa34a4d",
   "metadata": {},
   "source": [
    "Now let's build a pipeline for preprocessing the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "data_num_tr = num_pipeline.fit_transform(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e714f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(data_num)\n",
    "num_attribs.remove('Area_code')  # Area_Code is categorical although gets number\n",
    "cat_attribs = [\"State\", \"Area_code\", \"International_plan\", \"Voice_mail_plan\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "data_prepared = full_pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2aec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepared = data_prepared.toarray()\n",
    "data_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ea472",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "class OldDataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b59bd",
   "metadata": {},
   "source": [
    "Now let's join all these components into a big pipeline that will preprocess both the numerical and the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d205a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(data_num)\n",
    "num_attribs.remove('Area_code')\n",
    "cat_attribs = [\"State\", \"Area_code\", \"International_plan\", \"Voice_mail_plan\"]\n",
    "\n",
    "old_num_pipeline = Pipeline([\n",
    "        ('selector', OldDataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "old_cat_pipeline = Pipeline([\n",
    "        ('selector', OldDataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a42935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "old_full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", old_num_pipeline),\n",
    "        (\"cat_pipeline\", old_cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7820f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_prepared = old_full_pipeline.fit_transform(data)\n",
    "old_data_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77059ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66605eae",
   "metadata": {},
   "source": [
    "The result is the same as with the `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(data_prepared, old_data_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8fd50",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adeeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = model_selection.train_test_split(\n",
    "    data_prepared, data_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d82985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_val_test))\n",
    "print(len(y_train))\n",
    "print(len(y_val_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2c32c",
   "metadata": {},
   "source": [
    "## Test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = model_selection.train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfca77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_val))\n",
    "print(len(X_test))\n",
    "print(len(y_val))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd201dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67654",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"Total_calls\",\"Total_mins\",\"Total_charges\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs_0 = list(cat_encoder.categories_[0])\n",
    "cat_one_hot_attribs_1 = list(cat_encoder.categories_[1])\n",
    "cat_one_hot_attribs_2 = list(cat_encoder.categories_[2])\n",
    "cat_one_hot_attribs_3 = list(cat_encoder.categories_[3])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs_0 + cat_one_hot_attribs_1 + cat_one_hot_attribs_2 + cat_one_hot_attribs_3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b93b2",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b58ee",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab08e9",
   "metadata": {},
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(precision, recall, average_precision):\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')   \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047d23b",
   "metadata": {},
   "source": [
    "### Classification Score\n",
    "\n",
    "It gets the model and evaluates it for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_score(clf, X_train, y_train, X_val, y_val, train=True):\n",
    "    if train:\n",
    "        print(\"Train Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "\n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "\n",
    "    elif train == False:\n",
    "        print(\"Validation Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_val, clf.predict(X_val))))\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_val, clf.predict(X_val))\n",
    "        average_precision = average_precision_score(y_val, clf.predict(X_val))\n",
    "        plot_pr_curve(precision, recall, average_precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_val, clf.predict(X_val))\n",
    "        roc_auc = roc_auc_score(y_val, clf.predict(X_val))\n",
    "        print(\"roc auc score: {}\\n\".format(roc_auc))\n",
    "        plot_roc_curve(fpr, tpr, roc_auc)\n",
    "        \n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_val, clf.predict(X_val))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_val, clf.predict(X_val))))\n",
    "        plot_confusion_matrix(clf,  X_val, clf.predict(X_val))\n",
    "        print(\"End of validation Result\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdef50d",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(y_actual, y_pred):\n",
    "            \n",
    "        precision, recall, _ = precision_recall_curve(y_actual, y_pred)\n",
    "        average_precision = average_precision_score(y_actual, y_pred)\n",
    "        plot_pr_curve(precision, recall, average_precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_actual, y_pred)\n",
    "        roc_auc = roc_auc_score(y_actual, y_pred)\n",
    "        print(\"roc auc score: {}\\n\".format(roc_auc))\n",
    "        plot_roc_curve(fpr, tpr, roc_auc)\n",
    "        \n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_actual, y_pred)))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_actual, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95dbc03",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72126af",
   "metadata": {},
   "source": [
    "### Visualize support for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance(labels=[\"0\", \"1\"])\n",
    "\n",
    "visualizer.fit(y_train, y_val)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c6f00",
   "metadata": {},
   "source": [
    "## Balanced class weight - Combining Over Sampling and Under Sampling\n",
    "\n",
    "### SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_smoteen, y_train_smoteen = smote_enn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance(labels=[\"0\", \"1\"])\n",
    "\n",
    "visualizer.fit(y_train_smoteen)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26312936",
   "metadata": {},
   "source": [
    "# Select and Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b13d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing performance measure metrics\n",
    "import timeit\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8c336",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d00d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(dummy_clf, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd5092",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ab277",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(log_clf, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190383a",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(rf_clf, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d578124",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea77b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9571715",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "gb_clf.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(gb_clf, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016c5f7",
   "metadata": {},
   "source": [
    "## LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf = LGBMClassifier()\n",
    "lgbm_clf.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(gb_clf, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9fbbb",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b17026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e560212",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(xgb_clf, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ca070",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada_boost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_ada_boost.fit(X_train_smoteen, y_train_smoteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21680f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score(clf_ada_boost, X_train_smoteen, y_train_smoteen, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408f92d",
   "metadata": {},
   "source": [
    "### Evaluate model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d58ef5",
   "metadata": {},
   "source": [
    "#### Class Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b239e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "visualizer = ClassPredictionError(\n",
    "    clf_ada_boost, classes=classes, is_fitted=True\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cc9b4",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "visualizer = ClassificationReport(\n",
    "    clf_ada_boost, classes=classes, support=True, is_fitted=True\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "visualizer.show()                       # Finalize and show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c69db",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import confusion_matrix\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(clf_ada_boost, classes=[0,1], is_fitted=True)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86264da1",
   "metadata": {},
   "source": [
    "#### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "# Create the visualizer, fit, score, and show it\n",
    "viz = PrecisionRecallCurve(clf_ada_boost, is_fitted=True)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e0945",
   "metadata": {},
   "source": [
    "#### ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ROCAUC(\n",
    "    clf_ada_boost, classes=classes, is_fitted=True\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "visualizer.show()                       # Finalize and show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf9fd2",
   "metadata": {},
   "source": [
    "#### Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "viz = ValidationCurve(\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=42), \n",
    "    param_name=\"learning_rate\",\n",
    "    param_range=np.arange(1, 11), \n",
    "    cv=5, \n",
    "    scoring=\"f1_weighted\",\n",
    "    np_jobs=8\n",
    ")\n",
    "\n",
    "# Fit and show the visualizer\n",
    "viz.fit(X_train, y_train)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31def8c8",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "\n",
    "# Create the learning curve visualizer\n",
    "cv = StratifiedKFold(n_splits=12)\n",
    "sizes = np.linspace(0.3, 1.0, 10)\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "\n",
    "visualizer = LearningCurve(\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=42), \n",
    "    cv=cv, \n",
    "    scoring='f1_weighted', \n",
    "    train_sizes=sizes, \n",
    "    n_jobs=8\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train_smoteen, y_train_smoteen)        # Fit the data to the visualizer\n",
    "visualizer.poof()                       # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff577aad",
   "metadata": {},
   "source": [
    "#### Discrimintation Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import DiscriminationThreshold\n",
    "\n",
    "visualizer = DiscriminationThreshold(clf_ada_boost, is_fitted=True)\n",
    "\n",
    "visualizer.fit(X_train_smoteen, y_train_smoteen)\n",
    "visualizer.poof() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6617b",
   "metadata": {},
   "source": [
    "#### Decision Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# call decision_function on classifier to get scores (probas_pred)\n",
    "probas_pred = clf_ada_boost.decision_function(X_test)\n",
    "# compute precision-recall pairs for different probability thresholds\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, probas_pred)\n",
    "# precision and recall vs. the decision threshold\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a90855",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92241d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "fig = plt.figure(figsize=(22, 26))\n",
    "viz = FeatureImportances(clf_ada_boost, labels=attributes)\n",
    "viz.fit(X_train_smoteen, y_train_smoteen)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8af7d",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "A simple ANN architecture with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(12, activation='relu', input_shape=(113,)))\n",
    "mlp.add(Dense(8, activation= 'relu'))\n",
    "mlp.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.output_shape\n",
    "mlp.summary()\n",
    "mlp.get_config()\n",
    "mlp.get_weights()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77401506",
   "metadata": {},
   "source": [
    "mlp.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "train_y = np.asarray(y_train_smoteen)\n",
    "mlp.fit(X_train_smoteen, train_y, epochs=3, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f26fe5d6",
   "metadata": {},
   "source": [
    "y_pred = mlp.predict_classes(X_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2203db04",
   "metadata": {},
   "source": [
    "val_y = np.asarray(y_val)\n",
    "score = mlp.evaluate(X_val, val_y, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4b054fc",
   "metadata": {},
   "source": [
    "evaluation_metrics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e33aea",
   "metadata": {},
   "source": [
    "## TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92567f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51099b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, \n",
    "                      random_state=42, config_dict='TPOT light')\n",
    "tpot.fit(X_train_smoteen, y_train_smoteen)\n",
    "print(tpot.score(X_val, y_val))\n",
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f498bae",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
